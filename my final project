"""
sales_processor_project.py
==========================
ملف واحد يحوي المشروع كاملًا كما طلبت — قابل للفصل لاحقًا إلى ملفات/مجلدات.
Single-file monolith containing:
 - CLI (Typer)
 - Integrated processing pipeline (pandas-based)
 - PyQt6 GUI (embedded, no subprocess required)
 - Sample data generator and simple pytest-style tests
 - Example config YAML and README content embedded as strings
 - CI workflow snippet embedded as a string

ملاحظة مهمة: الصورة التي أرفعتها متاحة هنا (مسار محلي داخل الجلسة):
/mnt/data/Screenshot (67).png

يمكنك تفكيك هذا الملف إلى ملفات منفصلة بسهولة: كل قسم محفوف بعلامات
`# === FILE: <name> ===` لتساعدك على القص واللصق.

تشغيل سريع (بعد تثبيت المتطلبات):
    pip install pandas numpy matplotlib pyyaml typer[all] PyQt6
    python sales_processor_project.py --gui
    أو (CLI):
    python sales_processor_project.py --input examples/sample_sales.csv --output-dir out

الملف طويل لكنه منظم إلى أقسام. اقرأ التعليقات بالعربية والإنجليزية.
"""

# === FILE: imports_and_constants ===
from __future__ import annotations

import argparse
import csv
import json
import logging
import math
import os
import re
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml

# PyQt6 imports
try:
    from PyQt6 import QtCore, QtGui, QtWidgets
    PYQT_AVAILABLE = True
except Exception:
    PYQT_AVAILABLE = False

# Typer-like minimal CLI using argparse to avoid extra dependency in single file

DEFAULT_CONFIG = {
    "csv": {
        "chunksize": 100000,
        "encoding": "utf-8",
        "date_formats": [
            "%Y-%m-%d",
            "%d/%m/%Y",
            "%m/%d/%Y",
            "%Y/%m/%d",
            "%d-%m-%Y",
            "%Y.%m.%d",
        ],
        "parse_dates_column": "date",
        "columns": {"date": "date", "product_id": "product_id", "quantity": "quantity", "price": "price"},
    },
    "cleaning": {
        "strict_mode": False,
        "drop_bad_rows": True,
        "currency_symbols": ["$", "€", "£", "¥", "USD", "EUR"],
        "invalid_unicode_action": "replace",
    },
    "output": {"summary_filename": "summary.json", "aggregate_filename": "aggregate.csv", "plot_filename": "top_products.png"},
    "logging": {"level": "INFO"},
}

# === FILE: config_dataclass_and_utils ===
@dataclass
class Config:
    chunksize: int = 100000
    encoding: str = "utf-8"
    date_formats: List[str] = field(default_factory=lambda: DEFAULT_CONFIG["csv"]["date_formats"])  # type: ignore
    parse_dates_column: str = "date"
    columns: Dict[str, str] = field(default_factory=lambda: DEFAULT_CONFIG["csv"]["columns"])  # type: ignore
    strict_mode: bool = False
    drop_bad_rows: bool = True
    currency_symbols: List[str] = field(default_factory=lambda: DEFAULT_CONFIG["cleaning"]["currency_symbols"])  # type: ignore
    invalid_unicode_action: str = "replace"
    summary_filename: str = "summary.json"
    aggregate_filename: str = "aggregate.csv"
    plot_filename: str = "top_products.png"
    log_level: str = "INFO"

    @staticmethod
    def from_dict(d: Dict) -> "Config":
        csv_cfg = d.get("csv", {})
        cleaning = d.get("cleaning", {})
        output = d.get("output", {})
        logging_cfg = d.get("logging", {})
        return Config(
            chunksize=int(csv_cfg.get("chunksize", DEFAULT_CONFIG["csv"]["chunksize"])),
            encoding=str(csv_cfg.get("encoding", DEFAULT_CONFIG["csv"]["encoding"])),
            date_formats=csv_cfg.get("date_formats", DEFAULT_CONFIG["csv"]["date_formats"]),
            parse_dates_column=str(csv_cfg.get("parse_dates_column", DEFAULT_CONFIG["csv"]["parse_dates_column"])),
            columns=csv_cfg.get("columns", DEFAULT_CONFIG["csv"]["columns"]),
            strict_mode=bool(cleaning.get("strict_mode", DEFAULT_CONFIG["cleaning"]["strict_mode"])),
            drop_bad_rows=bool(cleaning.get("drop_bad_rows", DEFAULT_CONFIG["cleaning"]["drop_bad_rows"])),
            currency_symbols=cleaning.get("currency_symbols", DEFAULT_CONFIG["cleaning"]["currency_symbols"]),
            invalid_unicode_action=cleaning.get("invalid_unicode_action", DEFAULT_CONFIG["cleaning"]["invalid_unicode_action"]),
            summary_filename=output.get("summary_filename", DEFAULT_CONFIG["output"]["summary_filename"]),
            aggregate_filename=output.get("aggregate_filename", DEFAULT_CONFIG["output"]["aggregate_filename"]),
            plot_filename=output.get("plot_filename", DEFAULT_CONFIG["output"]["plot_filename"]),
            log_level=logging_cfg.get("level", DEFAULT_CONFIG["logging"]["level"]),
        )


def load_config_file(path: Optional[Path]) -> Config:
    if path is None:
        return Config.from_dict(DEFAULT_CONFIG)
    with open(path, "r", encoding="utf-8") as fh:
        raw = yaml.safe_load(fh) or {}
    return Config.from_dict(raw)


def setup_logging(level: str = "INFO") -> None:
    numeric = getattr(logging, level.upper(), logging.INFO)
    logging.basicConfig(level=numeric, format="%(asctime)s %(levelname)s: %(message)s")


# === FILE: cleaning_parsing ===

def try_rename_similar_columns(df_columns: List[str], expected: Dict[str, str]) -> Dict[str, str]:
    mapping: Dict[str, str] = {}
    normalized: Dict[str, str] = {}
    for c in df_columns:
        key = re.sub(r"\W+", "", c).lower()
        normalized[key] = c

    for exp_key, exp_name in expected.items():
        if exp_name in df_columns:
            mapping[exp_key] = exp_name
            continue
        for c in df_columns:
            if c.lower() == exp_name.lower():
                mapping[exp_key] = c
                break
        if exp_key in mapping:
            continue
        key_norm = re.sub(r"\W+", "", exp_name).lower()
        if key_norm in normalized:
            mapping[exp_key] = normalized[key_norm]
            continue
        for c in df_columns:
            if c.lower().startswith(exp_name[0:3].lower()):
                mapping[exp_key] = c
                break
    return mapping


def clean_currency_value(value: object, currency_symbols: List[str]) -> Optional[float]:
    if pd.isna(value):
        return None
    s = str(value)
    for sym in currency_symbols:
        s = s.replace(sym, "")
    s = s.replace(",", "").replace(" ", "").strip()
    s = re.sub(r"[^\d\.\-]", "", s)
    if s == "":
        return None
    try:
        return float(s)
    except Exception:
        return None


def parse_int_like(value: object) -> Optional[int]:
    if pd.isna(value):
        return None
    s = str(value).strip()
    s = re.sub(r"[^\d\-]", "", s)
    if s == "":
        return None
    try:
        return int(float(s))
    except Exception:
        return None


def parse_date_with_formats(value: object, formats: List[str]) -> Optional[pd.Timestamp]:
    if pd.isna(value):
        return None
    s = str(value).strip()
    for fmt in formats:
        try:
            dt = datetime.strptime(s, fmt)
            return pd.Timestamp(dt.date())
        except Exception:
            continue
    try:
        dt = pd.to_datetime(s, errors="coerce", utc=False)
        if pd.isna(dt):
            return None
        return pd.Timestamp(dt.date())
    except Exception:
        return None


# === FILE: aggregator ===
@dataclass
class Aggregator:
    total_sales: float = 0.0
    total_quantity: int = 0
    product_totals: Dict[str, float] = field(default_factory=lambda: defaultdict(float))
    product_quantities: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    rows_processed: int = 0
    rows_skipped: int = 0
    bad_rows: List[Dict] = field(default_factory=list)

    def ingest_row(self, product_id: str, quantity: int, price: float) -> None:
        self.rows_processed += 1
        self.total_quantity += int(quantity)
        revenue = float(quantity) * float(price)
        self.total_sales += revenue
        self.product_totals[product_id] += revenue
        self.product_quantities[product_id] += int(quantity)

    def record_bad_row(self, row: Dict, reason: str) -> None:
        self.rows_skipped += 1
        sample = dict(row)
        sample["_bad_reason"] = reason
        if len(self.bad_rows) < 1000:
            self.bad_rows.append(sample)

    def to_summary(self) -> Dict:
        top_products = sorted(self.product_totals.items(), key=lambda kv: kv[1], reverse=True)[:20]
        top_products_list = [
            {"product_id": pid, "revenue": rev, "quantity": self.product_quantities.get(pid, 0)} for pid, rev in top_products
        ]
        return {
            "generated_at": datetime.utcnow().isoformat() + "Z",
            "rows_processed": self.rows_processed,
            "rows_skipped": self.rows_skipped,
            "total_sales": round(float(self.total_sales), 2),
            "total_quantity": int(self.total_quantity),
            "unique_products": len(self.product_totals),
            "top_products": top_products_list,
            "bad_rows_sample": self.bad_rows,
        }

    def to_aggregate_dataframe(self) -> pd.DataFrame:
        items = [
            {"product_id": pid, "total_revenue": rev, "total_quantity": self.product_quantities.get(pid, 0)}
            for pid, rev in self.product_totals.items()
        ]
        df = pd.DataFrame(items)
        if df.empty:
            return df
        df = df.sort_values("total_revenue", ascending=False)
        return df


# === FILE: processing_pipeline ===

def process_csv(input_path: Path, cfg: Config, aggregator: Aggregator, sample_mode: bool = False) -> None:
    read_kwargs = {
        "chunksize": cfg.chunksize,
        "encoding": cfg.encoding,
        "dtype": str,
        "keep_default_na": False,
        "na_values": ["", "NA", "N/A", "null", "None"],
    }
    expected_cols = cfg.columns

    # quick header read
    with open(input_path, "r", encoding=cfg.encoding, errors="replace") as fh:
        sample_header = fh.readline().strip()
    header_cols = sample_header.split(",")
    preliminary_map = try_rename_similar_columns(header_cols, expected_cols)
    logging.debug("Preliminary column mapping: %s", preliminary_map)

    missing = [k for k in expected_cols.keys() if k not in preliminary_map]
    if missing and cfg.strict_mode:
        raise ValueError(f"Missing required columns (strict_mode): {missing}")

    for chunk_idx, chunk in enumerate(pd.read_csv(input_path, **read_kwargs)):
        logging.info("Processing chunk %d (rows: %d)", chunk_idx, len(chunk))
        col_map = try_rename_similar_columns(list(chunk.columns), expected_cols)
        logging.debug("Chunk column mapping: %s", col_map)
        missing_now = [k for k in expected_cols.keys() if k not in col_map]
        if missing_now and cfg.strict_mode:
            raise ValueError(f"[chunk {chunk_idx}] Missing required columns: {missing_now}")
        rename_map = {col_map[k]: k for k in col_map if col_map.get(k) is not None and col_map[k] != k}
        try:
            chunk = chunk.rename(columns=rename_map)
        except Exception:
            logging.debug("Rename map failed: %s", rename_map)

        for i, row in chunk.iterrows():
            if sample_mode and aggregator.rows_processed >= 1000:
                logging.info("Sample mode reached 1000 processed rows; stopping.")
                return
            if cfg.invalid_unicode_action == "replace":
                row = row.apply(lambda v: v.encode(cfg.encoding, errors="replace").decode(cfg.encoding, errors="replace") if isinstance(v, str) else v)
            elif cfg.invalid_unicode_action == "drop_row":
                def contains_invalid(s: object) -> bool:
                    if not isinstance(s, str):
                        return False
                    try:
                        s.encode(cfg.encoding)
                        return False
                    except Exception:
                        return True
                if any(contains_invalid(x) for x in row):
                    aggregator.record_bad_row(row.to_dict(), "invalid_unicode")
                    continue
            try:
                prod = row.get("product_id", None)
                qty_raw = row.get("quantity", None)
                price_raw = row.get("price", None)
                date_raw = row.get(cfg.parse_dates_column, None)
            except Exception:
                aggregator.record_bad_row(row.to_dict(), "missing_field_access")
                continue
            if prod is None or str(prod).strip() == "":
                aggregator.record_bad_row(row.to_dict(), "missing_product_id")
                continue
            prod = str(prod).strip()
            qty = parse_int_like(qty_raw)
            if qty is None:
                msg = "quantity_parse_failed"
                qty_try = parse_int_like(re.sub(r"[^\d\-]", "", str(qty_raw)))
                if qty_try is not None:
                    qty = qty_try
                    logging.warning("Cleaned quantity for product %s: %s -> %s", prod, qty_raw, qty)
                else:
                    aggregator.record_bad_row(row.to_dict(), msg)
                    continue
            price = clean_currency_value(price_raw, cfg.currency_symbols)
            if price is None or math.isnan(price):
                aggregator.record_bad_row(row.to_dict(), "price_parse_failed")
                continue
            parsed_date = parse_date_with_formats(date_raw, cfg.date_formats)
            if parsed_date is None and cfg.strict_mode:
                raise ValueError(f"Unable to parse date '{date_raw}' in strict_mode.")
            aggregator.ingest_row(prod, qty, price)


# === FILE: writers_and_plots ===

def write_summary_json(summary: Dict, outpath: Path) -> None:
    outpath.parent.mkdir(parents=True, exist_ok=True)
    with open(outpath, "w", encoding="utf-8") as fh:
        json.dump(summary, fh, ensure_ascii=False, indent=2)


def write_aggregate_csv(df: pd.DataFrame, outpath: Path) -> None:
    outpath.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(outpath, index=False, encoding="utf-8")


def plot_top_products(aggregator: Aggregator, outpath: Path, top_n: int = 10) -> None:
    df = aggregator.to_aggregate_dataframe().head(top_n)
    if df.empty:
        logging.warning("No products to plot.")
        return
    plt.figure(figsize=(10, 6))
    plt.barh(df["product_id"].astype(str), df["total_revenue"].astype(float))
    plt.xlabel("Total Revenue")
    plt.title(f"Top {len(df)} Products by Revenue")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    outpath.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(outpath)
    plt.close()


# === FILE: CLI_main ===

def cli_main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(prog="cli.py", description="Sales aggregator CLI")
    parser.add_argument("--input", required=False, help="Input CSV file path")
    parser.add_argument("--output-dir", required=False, default="out", help="Output directory")
    parser.add_argument("--config", required=False, help="Config YAML path")
    parser.add_argument("--sample", action="store_true", help="Sample mode")
    parser.add_argument("--gui", action="store_true", help="Start GUI instead of CLI")
    parser.add_argument("--log-level", default="INFO", help="Logging level")
    args = parser.parse_args(args=argv)

    if args.gui:
        # start GUI
        if not PYQT_AVAILABLE:
            print("PyQt6 is not available. Install PyQt6 to use the GUI.")
            return 1
        start_gui()
        return 0

    if not args.input:
        parser.print_help()
        print('\nError: Missing option "--input". Use --input <file.csv>')
        return 2

    input_path = Path(args.input)
    if not input_path.exists():
        print(f"Input file not found: {input_path}")
        return 3

    output_dir = Path(args.output_dir)
    cfg = load_config_file(Path(args.config) if args.config else None)
    cfg.log_level = args.log_level
    setup_logging(cfg.log_level)
    aggregator = Aggregator()
    try:
        process_csv(input_path, cfg, aggregator, sample_mode=args.sample)
    except Exception as e:
        logging.exception("Processing failed: %s", e)
        return 4

    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    write_summary_json(aggregator.to_summary(), out_dir / cfg.summary_filename)
    write_aggregate_csv(aggregator.to_aggregate_dataframe(), out_dir / cfg.aggregate_filename)
    try:
        plot_top_products(aggregator, out_dir / cfg.plot_filename)
    except Exception as e:
        logging.warning("Plot generation failed: %s", e)

    print("Done. Outputs in:", out_dir)
    return 0


# === FILE: PyQt_GUI_integration ===
if PYQT_AVAILABLE:
    class GUIWorker(QtCore.QObject):
        started = QtCore.pyqtSignal()
        finished = QtCore.pyqtSignal(int)
        log = QtCore.pyqtSignal(str)

        def __init__(self):
            super().__init__()
            self._thread: Optional[QtCore.QThread] = None

        def run_processing(self, input_csv: str, output_dir: str, config: Optional[str], sample: bool, log_level: str):
            # This runs on the main thread; to avoid blocking we will execute in a QThread wrapper below
            try:
                cfg = load_config_file(Path(config) if config else None)
                cfg.log_level = log_level
                setup_logging(cfg.log_level)
                agg = Aggregator()
                self.log.emit(f"Starting processing: {input_csv}")
                process_csv(Path(input_csv), cfg, agg, sample_mode=sample)
                out_dir = Path(output_dir)
                out_dir.mkdir(parents=True, exist_ok=True)
                write_summary_json(agg.to_summary(), out_dir / cfg.summary_filename)
                write_aggregate_csv(agg.to_aggregate_dataframe(), out_dir / cfg.aggregate_filename)
                try:
                    plot_top_products(agg, out_dir / cfg.plot_filename)
                except Exception as e:
                    self.log.emit(f"Plot failed: {e}")
                self.log.emit("Processing finished.")
                self.finished.emit(0)
            except Exception as e:
                self.log.emit(f"Error during processing: {e}")
                self.finished.emit(1)

    class MainWindow(QtWidgets.QMainWindow):
        def __init__(self):
            super().__init__()
            self.setWindowTitle("Sales Processor — GUI Integrated")
            self.resize(900, 700)
            self.worker = GUIWorker()
            self.worker_thread = QtCore.QThread()
            self.worker.moveToThread(self.worker_thread)
            self.worker_thread.start()

            central = QtWidgets.QWidget()
            self.setCentralWidget(central)
            layout = QtWidgets.QVBoxLayout(central)

            form = QtWidgets.QFormLayout()
            self.input_edit = QtWidgets.QLineEdit()
            btn = QtWidgets.QPushButton("Browse...")
            btn.clicked.connect(self.browse_input)
            h = QtWidgets.QHBoxLayout(); h.addWidget(self.input_edit); h.addWidget(btn)
            form.addRow("Input CSV:", h)

            self.output_edit = QtWidgets.QLineEdit(str(Path.cwd() / "out"))
            btn_out = QtWidgets.QPushButton("Browse...")
            btn_out.clicked.connect(self.browse_output)
            h2 = QtWidgets.QHBoxLayout(); h2.addWidget(self.output_edit); h2.addWidget(btn_out)
            form.addRow("Output Dir:", h2)

            self.config_edit = QtWidgets.QLineEdit()
            cfg_btn = QtWidgets.QPushButton("Browse...")
            cfg_btn.clicked.connect(self.browse_config)
            h3 = QtWidgets.QHBoxLayout(); h3.addWidget(self.config_edit); h3.addWidget(cfg_btn)
            form.addRow("Config (optional):", h3)

            options_h = QtWidgets.QHBoxLayout()
            self.sample_cb = QtWidgets.QCheckBox("Sample mode")
            self.log_combo = QtWidgets.QComboBox(); self.log_combo.addItems(["DEBUG","INFO","WARNING","ERROR"]) 
            options_h.addWidget(self.sample_cb); options_h.addStretch(); options_h.addWidget(QtWidgets.QLabel("Log level:")); options_h.addWidget(self.log_combo)
            form.addRow(options_h)

            layout.addLayout(form)

            btns = QtWidgets.QHBoxLayout()
            self.start_btn = QtWidgets.QPushButton("Start")
            self.stop_btn = QtWidgets.QPushButton("Stop"); self.stop_btn.setEnabled(False)
            self.start_btn.clicked.connect(self.start)
            self.stop_btn.clicked.connect(self.stop)
            btns.addWidget(self.start_btn); btns.addWidget(self.stop_btn); btns.addStretch()
            layout.addLayout(btns)

            self.log_view = QtWidgets.QPlainTextEdit(); self.log_view.setReadOnly(True)
            layout.addWidget(QtWidgets.QLabel("Logs")); layout.addWidget(self.log_view, stretch=2)

            self.progress = QtWidgets.QProgressBar(); self.progress.setRange(0,0); self.progress.setVisible(False)
            layout.addWidget(self.progress)

            self.image_label = QtWidgets.QLabel("Plot will appear here after processing.")
            self.image_label.setAlignment(QtCore.Qt.AlignmentFlag.AlignCenter)
            layout.addWidget(self.image_label, stretch=1)

            # connect worker signals
            self.worker.log.connect(self.append_log)
            self.worker.finished.connect(self.on_finished)

        def browse_input(self):
            path, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Select CSV", str(Path.cwd()), "CSV Files (*.csv);;All Files (*)")
            if path:
                self.input_edit.setText(path)

        def browse_output(self):
            path = QtWidgets.QFileDialog.getExistingDirectory(self, "Select output dir", str(Path.cwd()))
            if path:
                self.output_edit.setText(path)

        def browse_config(self):
            path, _ = QtWidgets.QFileDialog.getOpenFileName(self, "Select YAML", str(Path.cwd()), "YAML Files (*.yml *.yaml);;All Files (*)")
            if path:
                self.config_edit.setText(path)

        def start(self):
            inp = self.input_edit.text().strip()
            out = self.output_edit.text().strip()
            cfg = self.config_edit.text().strip() or None
            sample = self.sample_cb.isChecked()
            log_level = self.log_combo.currentText()
            if not inp or not Path(inp).exists():
                QtWidgets.QMessageBox.warning(self, "Input missing", "Please select a valid input CSV file.")
                return
            if not out:
                QtWidgets.QMessageBox.warning(self, "Output missing", "Please select an output directory.")
                return
            self.append_log("Starting processing...")
            self.progress.setVisible(True)
            self.start_btn.setEnabled(False); self.stop_btn.setEnabled(True)
            # run worker method in the worker thread
            QtCore.QMetaObject.invokeMethod(self.worker, "run_processing", QtCore.Qt.ConnectionType.QueuedConnection, QtCore.Q_ARG(str, inp), QtCore.Q_ARG(str, out), QtCore.Q_ARG(str, cfg if cfg else ""), QtCore.Q_ARG(bool, sample), QtCore.Q_ARG(str, log_level))

        def stop(self):
            # Not implemented graceful stop in integrated variant (would require cooperative cancellation)
            QtWidgets.QMessageBox.information(self, "Not Implemented", "Stop is not implemented in integrated mode. Use CLI subprocess mode for cancellable runs.")

        def append_log(self, text: str):
            self.log_view.appendPlainText(text)

        def on_finished(self, code: int):
            self.append_log(f"Finished with code {code}")
            self.progress.setVisible(False)
            self.start_btn.setEnabled(True); self.stop_btn.setEnabled(False)
            out = Path(self.output_edit.text().strip())
            plot = out / DEFAULT_CONFIG["output"]["plot_filename"]
            if plot.exists():
                pix = QtGui.QPixmap(str(plot))
                if not pix.isNull():
                    scaled = pix.scaled(self.image_label.size(), QtCore.Qt.AspectRatioMode.KeepAspectRatio, QtCore.Qt.TransformationMode.SmoothTransformation)
                    self.image_label.setPixmap(scaled)

# === FILE: sample_and_tests ===

def create_sample_csv(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8", newline="") as fh:
        writer = csv.writer(fh)
        writer.writerow(["date", "product_id", "quantity", "price"])
        writer.writerow(["2025-01-05", "P123", "2", "19.99"])
        writer.writerow(["2025-01-06", "P124", "1", "5.00"])
        writer.writerow(["2025-01-06", "P123", "3", "19.99"])


def run_simple_test():
    # simple local test that emulates AT1
    sample = Path("examples/sample_sales.csv")
    create_sample_csv(sample)
    out = Path("examples/out")
    cfg = load_config_file(None)
    setup_logging("INFO")
    agg = Aggregator()
    process_csv(sample, cfg, agg, sample_mode=False)
    summary = agg.to_summary()
    # basic assertions
    assert summary["rows_processed"] == 3
    assert round(summary["total_sales"],2) == round(2*19.99 + 1*5.0 + 3*19.99,2)
    print("Simple test passed")


# === FILE: README_and_CI_snippets ===
README = """
Sales Processor
===============
Single-file project. Use as CLI or GUI.

Quick start:

    pip install pandas numpy matplotlib pyyaml PyQt6
    python sales_processor_project.py --input examples/sample_sales.csv --output-dir out

Or run GUI:

    python sales_processor_project.py --gui

Note: You uploaded an image during the session; path on the system is:
/mnt/data/Screenshot (67).png

You can split this file into modules by cutting at the markers `# === FILE: name ===`.
"""

CI_YAML = """
# Example GitHub Actions workflow (ci.yml)
# Runs black --check, flake8, pytest
name: CI
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest -q
"""

# === FILE: main_entrypoint ===

def start_gui():
    if not PYQT_AVAILABLE:
        print("PyQt6 not installed. Install with: pip install PyQt6")
        return
    app = QtWidgets.QApplication(sys.argv)
    app.setStyle("Fusion")
    win = MainWindow()
    win.show()
    app.exec()


if __name__ == "__main__":
    # if run directly act as CLI
    code = cli_main(sys.argv[1:])
    sys.exit(code)
